{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment DeepSeek: Step-by-Step Guide\n",
        "\n",
        "This guide demonstrates how to deploy [deepseek/DeepSeek-R1-Distill-Qwen-32B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) with Vast's templates and integrate them with Langchain for advanced processing capabilities."
      ],
      "metadata": {
        "id": "4r_LZEMTh4nr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "aZcHfzRZhdII"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "d_L0B-A4hSd0"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --upgrade vastai\n",
        "!pip install --upgrade langchain langchain-openai openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "export VAST_API_KEY=\"405acbff24f03c3dcade754ed546406ac41b8fa01df72a5726e6383a896bb\"\n",
        "vastai set api-key $VAST_API_KEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXhwibzQGoQQ",
        "outputId": "c5eb85d1-b4d3-46a2-a211-fd3788a083dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your api key has been saved in /root/.config/vastai/vast_api_key\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing Hardware\n",
        "\n",
        "To deploy the DeepSeek-R1-Distill-Qwen-32B model on Vast.ai, we need to find a GPU with the following specifications:\n",
        "\n",
        "1. GPU Memory:\n",
        "  - DeepSeek model weights (32B Parameters)\n",
        "  - KV Cache for handling of extra long output token lengths\n",
        "\n",
        "\n",
        "2. At least one direct port that we can forward for:\n",
        "   - vLLM's OpenAI-compatible API server\n",
        "   - External access to the model endpoint\n",
        "   - Secure request routing\n",
        "\n",
        "3. At least 120GB of disk space to hold the model and other things we might like to download"
      ],
      "metadata": {
        "id": "ytpayLBAHY9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "vastai search offers \"compute_cap >= 750 \\\n",
        "gpu_ram >= 80 \\\n",
        "num_gpus = 1 \\\n",
        "static_ip = true \\\n",
        "direct_port_count >= 1 \\\n",
        "verified = true \\\n",
        "disk_space >= 120 \\\n",
        "rentable = true\""
      ],
      "metadata": {
        "id": "0TicTq5ekjX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c87624d-fb81-48ea-ed87-05631633b87d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID        CUDA   N  Model      PCIE  cpu_ghz  vCPUs    RAM  Disk  $/hr    DLP    DLP/$   score  NV Driver   Net_up  Net_down  R     Max_Days  mach_id  status    host_id  ports  country       \n",
            "15952834  12.2  1x  H100_SXM   54.9  3.8      28.0   193.5  433   2.0681  345.6  167.12  321.9  535.216.03  6806.7  8125.4    99.7  30.0      31688    verified  68137    1249   France,_FR    \n",
            "18284030  12.4  1x  H200       48.9  4.0      24.0   258.0  2386  3.2009  454.4  141.97  319.3  550.127.05  4346.1  7118.5    99.7  122.2     32676    verified  97732    4999   ,_US          \n",
            "17618585  12.2  1x  H100_SXM   54.9  3.8      32.0   221.1  701   1.9344  345.4  178.55  311.0  535.230.02  7211.0  8410.4    99.2  8.8       33035    verified  68137    1428   France,_FR    \n",
            "17201721  12.6  1x  H100_SXM   52.5  4.1      32.0   128.9  1649  1.8682  345.9  185.16  305.5  560.35.05   2008.3  11354.3   99.4  151.8     32847    verified  125728   249    Czechia,_CZ   \n",
            "16461479  12.6  1x  H100_SXM   54.9  3.7      24.0   290.2  331   1.6708  345.1  206.55  264.9  560.35.05   754.7   645.0     99.6  90.4      32302    verified  169960   2047   India,_IN     \n",
            "17645793  12.6  1x  H100_SXM   54.9  3.7      24.0   290.2  553   1.5908  345.5  217.21  261.5  560.35.03   596.3   600.1     99.2  29.2      33102    verified  169960   2047   India,_IN     \n",
            "12431899  12.7  1x  H100_NVL   53.3  3.7      8.0    193.5  2375  2.2681  311.1  137.14  258.4  565.57.01   7281.4  7580.7    99.9  670.3     27272    verified  135125   249    California,_US\n",
            "13885157  12.7  1x  H100_NVL   54.9  3.7      8.0    193.5  697   2.4014  319.6  133.10  257.2  565.57.01   6363.1  7047.6    99.7  336.6     30393    verified  135125   249    California,_US\n",
            "12188926  12.7  1x  H100_NVL   54.5  3.7      8.0    193.5  1971  2.5347  311.7  122.96  238.4  565.57.01   6576.6  6865.3    99.7  670.3     26134    verified  135125   249    California,_US\n",
            "13690958  12.5  1x  A100_SXM4  24.2  -        32.0   128.9  1298  1.0947  127.4  116.40  209.2  555.52.04   6879.6  7874.9    99.9  151.7     25723    verified  125728   24     Czechia,_CZ   \n",
            "17440541  12.4  1x  H100_NVL   54.9  3.8      16.0   193.3  756   2.3347  352.7  151.06  206.9  550.127.08  688.7   844.1     99.8  51.7      31085    verified  173452   249    New_York,_US  \n",
            "15542752  12.4  1x  H100_SXM   52.8  2.8      16.0   258.0  2419  2.4014  306.2  127.49  201.0  550.127.08  1217.5  5529.7    99.7  337.0     31687    verified  135125   2047   ,_US          \n",
            "13482713  12.6  1x  H100_SXM   36.8  3.7      16.0   128.9  250   2.1347  228.2  106.92  193.7  560.35.03   3136.1  6655.1    99.9  151.1     27587    verified  145222   691    Thailand,_TH  \n",
            "17830358  12.4  1x  H100_PCIE  50.2  3.7      37.3   171.9  928   1.4269  244.2  171.17  170.1  550.127.08  883.9   872.2     96.8  3.8       33012    verified  184705   133    Texas,_US     \n",
            "17244203  12.7  1x  H100_NVL   53.8  3.7      4.0    193.5  3645  2.4014  234.9  97.82   167.7  565.57.01   7290.5  7842.9    98.8  367.4     30392    verified  135125   249    California,_US\n",
            "16848092  12.4  1x  H200       52.7  -        12.0   386.5  623   3.4681  255.6  73.70   141.4  550.127.08  2065.5  6067.9    99.4  27.0      32503    verified  68137    1249   France,_FR    \n",
            "17667590  12.6  1x  A100_SXM4  23.2  -        24.0   257.9  219   1.4681  122.1  83.20   137.5  560.35.03   1737.7  7380.2    99.3  152.2     32761    verified  158207   4095   ,_US          \n",
            "18242683  12.6  1x  A100_SXM4  24.4  -        24.0   257.9  151   1.4681  122.1  83.19   111.9  560.35.03   776.8   885.5     99.8  274.1     27828    verified  158207   4095   ,_US          \n",
            "17782482  12.6  1x  A100_SXM4  21.2  -        24.0   257.9  307   1.4681  122.1  83.20   109.8  560.35.03   1580.7  6369.9    97.1  152.1     31704    verified  158207   4095   ,_US          \n",
            "18355962  12.8  1x  A100_SXM4  21.2  3.7      32.0   128.9  2460  1.0414  131.9  126.69  108.1  570.86.16   3341.3  5507.5    88.8  151.7     33686    verified  125728   24     Czechia,_CZ   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploying the Server via Vast Template\n",
        "\n",
        "Choose a machine and copy and paste the id below to set `INSTANCE_ID`.\n",
        "\n",
        "We will deploy a template that:\n",
        "1. Uses `vllm/vllm-openai:latest` docker image. This gives us an OpenAI-compatible server.\n",
        "2. Forwards port `8000` to the outside of the container, which is the default OpenAI server port\n",
        "3. Forwards `--model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --max-model-len 8192  --enforce-eager` on to the default entrypoint (the server itself)\n",
        "4. Uses `--tensor-parallel-size 1` by default.\n",
        "5. Uses `--gpu-memory-utilization 0.90` by default\n",
        "6. Ensures that we have 120 GB of Disk space\n",
        "\n",
        "These settings balance performance and stability for serving the DeepSeek model."
      ],
      "metadata": {
        "id": "PWTcVEyJFI-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "export INSTANCE_ID='13690958'\n",
        "vastai create instance $INSTANCE_ID --disk 120 --template_hash eda062b3e0c9c36f09d9d9a294405ded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4BwtJJrIhq7",
        "outputId": "2e7d819c-4a52-4a50-ae1d-1f9abde366e3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started. {'success': True, 'new_contract': 18385217}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify Setup"
      ],
      "metadata": {
        "id": "BiuiZDPPItRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "export VAST_IP_ADDRESS=\"80.188.223.202\"\n",
        "export VAST_PORT=\"11459\"\n",
        "curl -X POST http://$VAST_IP_ADDRESS:$VAST_PORT/v1/completions \\\n",
        "     -H \"Content-Type: application/json\" \\\n",
        "     -d '{\n",
        "           \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n",
        "           \"prompt\": \"Hello, how are you?\",\n",
        "           \"max_tokens\": 50\n",
        "         }'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noYlbGedIqnf",
        "outputId": "c814a711-abc1-4ee9-d819-a80a76c6ccc5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"id\":\"cmpl-f6d723bc5b32444a8be49d8c20cb5b9c\",\"object\":\"text_completion\",\"created\":1740837657,\"model\":\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\"choices\":[{\"index\":0,\"text\":\" Can you help me with my assignment? I need to analyze this passage:\\n\\n\\\"Treason doth never prosper: what's the reason?\\nWhy, if it prosper, none dare call it treason.\\\"\\n\\nCan you explain the meaning of this passage? What is\",\"logprobs\":null,\"finish_reason\":\"length\",\"stop_reason\":null,\"prompt_logprobs\":null}],\"usage\":{\"prompt_tokens\":7,\"total_tokens\":57,\"completion_tokens\":50,\"prompt_tokens_details\":null}}"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   148    0     0  100   148      0    123  0:00:01  0:00:01 --:--:--   123\r100   148    0     0  100   148      0     67  0:00:02  0:00:02 --:--:--    67\r100   734  100   586  100   148    233     58  0:00:02  0:00:02 --:--:--   292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yPMOu5xdIqpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage"
      ],
      "metadata": {
        "id": "QQkJ6q42FJGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a DeepSeek Output Parser"
      ],
      "metadata": {
        "id": "iAZ0Qi0aJ2d6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeepSeek's response contains two parts:\n",
        "1. Thinking - wrapped in `<think>` tags\n",
        "2. Answer - follows after the tags\n"
      ],
      "metadata": {
        "id": "rDDYb7jzKLjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "from typing import Optional, Tuple\n",
        "from langchain.schema import BaseOutputParser\n",
        "\n",
        "class R1OutputParser(BaseOutputParser[Tuple[Optional[str], str]]):\n",
        "    \"\"\"Parser for DeepSeek R1 model output that includes thinking and response sections.\"\"\"\n",
        "\n",
        "    def parse(self, text: str) -> Tuple[Optional[str], str]:\n",
        "        \"\"\"Parse the model output into thinking and response sections.\n",
        "\n",
        "        Args:\n",
        "            text: Raw text output from the model\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing (thinking_text, response_text)\n",
        "            - thinking_text will be None if no thinking section is found\n",
        "        \"\"\"\n",
        "        if \"</think>\" in text:\n",
        "            # Split on </think> tag\n",
        "            parts = text.split(\"</think>\")\n",
        "            # Extract thinking text (remove <think> tag)\n",
        "            thinking_text = parts[0].replace(\"<think>\", \"\").strip()\n",
        "            # Get response text\n",
        "            response_text = parts[1].strip()\n",
        "            return thinking_text, response_text\n",
        "\n",
        "        # If no thinking tags found, return None for thinking and full text as response\n",
        "        return None, text.strip()\n",
        "\n",
        "    @property\n",
        "    def _type(self) -> str:\n",
        "        \"\"\"Return type key for serialization.\"\"\"\n",
        "        return \"r1_output_parser\""
      ],
      "metadata": {
        "id": "wdD8mAcaJ0CU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Model"
      ],
      "metadata": {
        "id": "bec8DYdYKUy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VAST_IP_ADDRESS=\"80.188.223.202\"\n",
        "VAST_PORT=\"11459\"\n",
        "\n",
        "openai_api_key = \"EMPTY\"\n",
        "openai_api_base = f\"http://{VAST_IP_ADDRESS}:{VAST_PORT}/v1\"\n",
        "\n",
        "model = ChatOpenAI(\n",
        "    base_url=openai_api_base,\n",
        "    api_key=openai_api_key,\n",
        "    model_name=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n",
        "    max_tokens=8000,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# Create prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Create parser\n",
        "parser = R1OutputParser()\n",
        "\n",
        "# Create chain\n",
        "chain = (\n",
        "    {\"input\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")"
      ],
      "metadata": {
        "id": "smgfvaa7J8Pj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Request"
      ],
      "metadata": {
        "id": "6MFdQRjBKneb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = \"Explain CPU to a 10-year-old who loves AI.\"\n",
        "\n",
        "thinking, response = chain.invoke(prompt_text)\n",
        "print(\"\\nTHINKING:\\n\")\n",
        "print(thinking)\n",
        "print(\"\\nRESPONSE:\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXSUT5fZKYgD",
        "outputId": "b0a9cb8c-1ee9-4022-9b77-0c6e72fedca5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "THINKING:\n",
            "\n",
            "Okay, so I need to explain what a CPU is to a 10-year-old who loves AI. Hmm, let's think about how to approach this. First, I should make sure I understand what a CPU is myself. From what I know, CPU stands for Central Processing Unit, and it's often called the \"brain\" of the computer. It's responsible for executing instructions and performing calculations. But how do I translate that into something a kid who's into AI would understand?\n",
            "\n",
            "Maybe I should relate it to something they already know. Since they love AI, perhaps I can compare the CPU to something in AI. Wait, but AI is more about algorithms and data processing. Maybe I should think of the CPU as the part that makes AI work? So, if the CPU is the brain, then in the context of AI, it's the part that helps the AI think and make decisions.\n",
            "\n",
            "Let me break it down. The CPU is like the worker inside the computer who does all the tasks. When you tell the AI to do something, like answer a question or play a game, the CPU is the one that makes it happen. It takes the instructions given by the AI's software and executes them. Without the CPU, the AI wouldn't be able to process anything or learn from data.\n",
            "\n",
            "I should also mention that the CPU is super fast. It does billions of calculations every second, which is why AI can respond quickly or learn from lots of data in a short time. Maybe use an analogy they can relate to, like a superhero or something. The CPU is like a superhero inside the computer, doing all the hard work so the AI can be smart and helpful.\n",
            "\n",
            "I need to keep it simple and avoid technical jargon. Maybe use terms like \"worker,\" \"brain,\" or \"superhero.\" Also, include examples they might be familiar with, like playing a video game or using a voice assistant. The CPU is what makes those things work smoothly with AI.\n",
            "\n",
            "Wait, but I shouldn't forget that the CPU works with other parts too, like the GPU. But maybe that's too detailed for a 10-year-old. I should focus on the CPU's role in making AI function. So, the CPU takes the AI's instructions, processes data, and makes decisions, all at lightning speed.\n",
            "\n",
            "Let me structure this: Start by comparing the CPU to something familiar, like a brain or a worker. Explain that it's the part that makes AI think and respond. Mention how fast it is and how it handles many tasks at once. Use an analogy they can relate to, maybe a superhero or a very fast thinker.\n",
            "\n",
            "I should also make it engaging, maybe with a fun example. Like, when you ask an AI to solve a math problem, the CPU quickly figures it out because it's so fast and smart. Or when an AI plays a game, the CPU helps it make the right moves by processing information quickly.\n",
            "\n",
            "I think I have a good outline. Now, I'll put it all together in a simple, kid-friendly way. Make sure it's short and uses relatable terms without getting too technical.\n",
            "\n",
            "RESPONSE:\n",
            "\n",
            "Sure! Imagine the CPU as the superhero inside your computer. Just like a superhero has superpowers, the CPU has the power to make your computer and AI do amazing things! When you ask the AI to solve a problem or play a game, the CPU is the one who quickly processes everything, like a superhero in a fast-paced action movie. It's super fast, doing billions of tasks every second, which is why the AI can answer questions and learn so quickly. Without the CPU, the AI wouldn't be able to think or respond, just like a superhero without their powers. So, the CPU is the hero that makes AI smart and fun!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=openai_api_key,\n",
        "    base_url=openai_api_base,\n",
        ")\n",
        "\n",
        "chat_response = client.chat.completions.create(\n",
        "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": \"Hello, how are you today\"},\n",
        "        ],\n",
        "    }],\n",
        ")\n",
        "print(\"Chat completion output:\", chat_response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X43j7noIPNhO",
        "outputId": "69674cbe-19ac-4f92-c26e-f1b4f1a7f41b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat completion output: Alright, someone just said, \"Hello, how are you today.\" I need to respond to that.\n",
            "\n",
            "Since I'm an AI, I don't have feelings, but I should acknowledge that.\n",
            "\n",
            "I'll start by saying I don't have feelings, then ask how they're doing.\n",
            "\n",
            "Keeping it friendly and open for them to continue the conversation.\n",
            "</think>\n",
            "\n",
            "Hello! I'm just a computer program, so I don't have feelings, but thanks for asking! How are you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delete Machine"
      ],
      "metadata": {
        "id": "738sNUgoMad8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete vast.ai machine\n",
        "%%bash\n",
        "export INSTANCE_ID='13690958'\n",
        "vastai destroy instance $INSTANCE_ID"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsGU2cDlKsyg",
        "outputId": "75d8fbb1-0968-4a26-fd14-0c558086f01f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "failed with error 404: Instance 13690958 not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yz1NU8RFPkn1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}